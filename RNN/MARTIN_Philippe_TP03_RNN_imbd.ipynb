{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBFqSEkKqpCN"
   },
   "source": [
    "# TP RNN \n",
    "# Using Many-to-One for movie rating predicton\n",
    "\n",
    "For any remark or suggestion, please feel free to contact me at:\n",
    "geoffroy.peeters@telecom-paristech.fr\n",
    "\n",
    "Last edit: 2019/01/15 geoffroy.peeters@telecom-paristech.fr\n",
    "\n",
    "### Objective:\n",
    "We will implement two different networks to perform automatic rating (0 or 1) of a movie given the text of its review.\n",
    "We will use the ```imdb``` (internet movie database) dataset.\n",
    "\n",
    "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmkCSNaXLqjh"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOqjzDwioJj9"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5Yp4OQVvUtr"
   },
   "source": [
    "## Parameters of the model\n",
    "\n",
    "-  We only consider the ```top_words``` first words in the word dictionary\n",
    "- We truncate/zerp-pad each sequence a length ```max_review_length````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4C_Pv7rYvRkM"
   },
   "outputs": [],
   "source": [
    "top_words = 5000 \n",
    "max_review_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsNcRimyLzgP"
   },
   "source": [
    "## Import IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Gfe1ex8oN8Q"
   },
   "outputs": [],
   "source": [
    "# Import the IMDB data and only consider the ``top_words``` most used words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FSywXji-6-j7",
    "outputId": "eaf089bf-30f6-4181-e837-9fa7ade3c4f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSc5LmksOLyr"
   },
   "source": [
    "## Data content\n",
    "\n",
    "- ```X_train``` and ```X_test``` are numpy arrays of lists. \n",
    "  - each item in a list is the index in the word dictionary. So that a list is the sequence of index of words.\n",
    "\n",
    "- ```y_train``` and ```y_test``` are a numpy arrays of the same dimension as ```X_train``` and ```X_test``` \n",
    "  - they contains the values 0 (bad movie) or 1 (good movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "WouODCPrtiuu",
    "outputId": "449f7aa2-33c7-43b5-cf8a-8cc7c0b5dc27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_train): <class 'numpy.ndarray'>\n",
      "number of training sequences: X_train.shape: (25000,)\n",
      "type(X_train[0]): <class 'list'>\n",
      "length of the first training sequence: len(X_train[0]): 218\n",
      "length of the second training sequence: len(X_train[0]): 189\n",
      "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "maximum length of a training sequence: 2494\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"type(X_train):\", type(X_train))\n",
    "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
    "print(\"type(X_train[0]):\",type(X_train[0]))\n",
    "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
    "print(\"length of the second training sequence: len(X_train[0]):\",len(X_train[1]))\n",
    "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
    "len_list = [len(train) for train in X_train]\n",
    "print(\"maximum length of a training sequence:\", max(len_list))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(len_list, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Hfl42LGCugWB",
    "outputId": "7a99fbe6-220d-4fd4-90ab-df10ac096486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(y_train): <class 'numpy.ndarray'>\n",
      "y_train.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(y_train):\", type(y_train))\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iVw65PNNuobX",
    "outputId": "5964d29e-2d18-4c53-ebf4-ef347631e805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (25000,)\n",
      "y_test.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V18OA7oQNH3c"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "Sequences (represented as a list of values) in ```X_train``` represent the reviews.\n",
    "They can have different length.\n",
    "To train the network we should modify them so that they all have the same length.\n",
    "We do this by:\n",
    "- truncating the ones that are too long\n",
    "- padding-with-zero them the ones that are too short.\n",
    "\n",
    "This is obtained using ```sequence.pad_sequences``` of keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "JhmiHsOGoRwT",
    "outputId": "c00c880b-a48d-45ac-f9ae-aa11014fbda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
      "    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n",
      "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
      "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
      "    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n",
      "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
      "   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n",
      "  178   32]\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "\n",
    "# --- START CODE HERE\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlrDTuk5K65Q"
   },
   "source": [
    "## First model\n",
    "\n",
    "In the first model, we will simply \n",
    "- learn a word embedding  (```Embedding``` layer in keras) and apply it to each of item of a sequence, \n",
    "  -  in keras, embedding is not a matrix going from one-hot-encoding to embedding, it is a layer that goes from index-in-word-dictionary to embedding\n",
    "  - the embedding goes from a catalogue of words of size```top_words``` dimensions to  ```embedding_vector_length``` dimensions\n",
    "- average the embedding obtained for each word of a seuqnece over all words of the sequence (you should use ```K.mean``` from the keras backend https://keras.io/backend/ and ```Lambda``` https://keras.io/layers/core/ ) \n",
    "- apply a fully connected (```Dense``` layer in keras) which output activation is a sigmoid ()predicting the 0 or 1 rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufW00TGcs3Jj"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "zspaUptgtW9l",
    "outputId": "184b26a2-893c-4e7a-8420-587f018d9d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32 \n",
    "\n",
    "#Le r√©seau va sortir a(Tx)\n",
    "#(m,Tx,nx) : on ne connait pas m le nb d'exemple : Tx = dim temporelle et nx dim de chaque variable\n",
    "#(m,Tx,nx) = (None, 100, 2994)\n",
    "# --- START CODE HERE\n",
    "X = Input(shape=(max_review_length,))\n",
    "X1 = Embedding(input_dim = top_words, output_dim = embedding_vector_length,input_length=max_review_length)(X) #va donner (m,Tx,embedding_vector_length)\n",
    "X2 = Lambda(lambda x: K.mean(x, axis=1))(X1)\n",
    "Y = Dense(1,activation = 'sigmoid')(X2)\n",
    "model = Model(inputs = X, outputs = Y)\n",
    "# --- END CODE HERE\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "pFXz4AS6tawQ",
    "outputId": "8bd92e7c-2772-473e-a12e-afdd2b740d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 139us/step - loss: 0.6357 - acc: 0.7304 - val_loss: 0.5592 - val_acc: 0.7774\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 3s 111us/step - loss: 0.4844 - acc: 0.8139 - val_loss: 0.4389 - val_acc: 0.8231\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 3s 106us/step - loss: 0.3931 - acc: 0.8455 - val_loss: 0.3855 - val_acc: 0.8359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff150b08080>"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBqyzLJRUIsC"
   },
   "source": [
    "##Results\n",
    "\n",
    "After only 3 epochs, you should obtain an accuracy around 84% for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK9e5Eo1Ks2a"
   },
   "source": [
    "## Second model\n",
    "\n",
    "In the second model, we will replace\n",
    "- average the obtained embedding over the sequence (use ```K.mean``` and ```Lambda```from keras backend)\n",
    "- by a RNN layer (more precisely an ```LSTM```) in a Many-To-One configuration with $n_a=100$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwoXuOqqVDOy"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "7dl-CSMKoViX",
    "outputId": "67bbe4a3-f5b9-47b5-9daf-e847fd1e4c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"embedding_1/embedding_lookup/Identity:0\", shape=(?, 100, 32), dtype=float32)\n",
      "Tensor(\"lstm_1/TensorArrayReadV3:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"dense_1/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "\n",
    "# --- START CODE HERE\n",
    "X = Input(shape=(max_review_length,))\n",
    "print(X)\n",
    "X1 = Embedding(input_dim = top_words, output_dim = embedding_vector_length,input_length=max_review_length)(X) #va donner (m,Tx,embedding_vector_length)\n",
    "print(X1)\n",
    "X2 = LSTM(100, return_sequences = False)(X1) #False permet de faire le many to one\n",
    "print(X2)\n",
    "Y = Dense(1, activation = 'sigmoid')(X2)\n",
    "print(Y)\n",
    "model = Model(inputs = X, outputs = Y)\n",
    "# --- END CODE HERE\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "-bp7PzX7oXtB",
    "outputId": "bd918ceb-065a-42ca-97cf-406ab09392a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 73s 3ms/step - loss: 0.4557 - acc: 0.7735 - val_loss: 0.3628 - val_acc: 0.8412\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 70s 3ms/step - loss: 0.3034 - acc: 0.8754 - val_loss: 0.3436 - val_acc: 0.8537\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.2539 - acc: 0.8984 - val_loss: 0.3505 - val_acc: 0.8514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff14296f128>"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1LN_fjMWBHJ"
   },
   "source": [
    "## Results\n",
    "\n",
    "After only 3 epochs, you should obtain an accuracy around 88% for the test data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP03_RNN_imbd_fot_students.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
