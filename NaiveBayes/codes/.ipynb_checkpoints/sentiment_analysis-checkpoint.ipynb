{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "\n",
    "# Loading datasets\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('..', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('..', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "filenames_stop_words = sorted(glob(op.join('..','data','english.stop')))\n",
    "\n",
    "# Get the list of stop words\n",
    "\n",
    "########################################\n",
    "#### CHANGE HERE FOR THE STOP_WORDS ####\n",
    "#stop_words = [open(f).read() for f in filenames_stop_words][0].split() #question 5\n",
    "stop_words = []\n",
    "########################################\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Define labeled texts\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctionation_and_stopwords(text_list) :\n",
    "    \"\"\"Process text : Remove punctiation and stop words from a list of words\"\"\"\n",
    "    exclude = set(string.punctuation)\n",
    "    new_text = [word for word in text_list if word not in exclude and word not in stop_words]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(texts):\n",
    "    \"\"\"Return a set of all the word in a corpus of pre-processed texts\"\"\"\n",
    "    vocabulary = set()\n",
    "    for text in texts:\n",
    "        text = remove_punctionation_and_stopwords(text.split())\n",
    "        for word in text :\n",
    "            vocabulary.add(word)\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    print(\"Processing corpus of texts\")\n",
    "    vocabulary_set = list(get_vocabulary(texts))\n",
    "    counts = np.zeros((len(texts),len(vocabulary_set)))\n",
    "    vocabulary = {}\n",
    "    for i, elem in enumerate(vocabulary_set) :\n",
    "        vocabulary[elem] = i\n",
    "    for i in range(len(texts)) :\n",
    "        text = remove_punctionation_and_stopwords(texts[i].split())\n",
    "        for word in text :\n",
    "            counts[i,vocabulary[word]] = counts[i,vocabulary[word]] + 1\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDocsInClass(y,c) :\n",
    "    \"\"\" return the number of docs that are labeled c \"\"\"\n",
    "    return np.sum((y==c).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenateTextOfAllDocsInClass(X,y,c) :\n",
    "    \"\"\" return the number of time each word was found in the corpus X of texts that are labeled c \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : count_words(texts)[1]\n",
    "    y : array of labels\n",
    "    c : label (\"class\") {0,1}\n",
    "    Returns\n",
    "    -------\"\"\"\n",
    "    assert X.shape[0] == len(y)\n",
    "    concatenated = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[0]) :\n",
    "        if int(y[i]) == c :\n",
    "            concatenated += X[i,:]\n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countTokensOfTerm(hist_of_words_c,word_index) :\n",
    "    \"\"\" return the number of time a token appeared in a corpus of texts that are labeled c\"\"\"\n",
    "    return hist_of_words_c[word_index]\n",
    "\n",
    "def extractTokensFromDoc(doc) :\n",
    "    \"\"\" return all tokens that appear at least once in the doc\"\"\"\n",
    "    return np.nonzero(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, allClasses = [0,1]):\n",
    "        self.nDocs = len(texts)\n",
    "        self.prior = [] # P(class)\n",
    "        self.condProb = np.zeros((len(vocabulary),len(allClasses))) # initialize P(word|class)\n",
    "\n",
    "    def fit(self, X, y, allClasses = [0,1]):\n",
    "        print(\"Training model ...\")\n",
    "        for c in allClasses :\n",
    "            Nc = countDocsInClass(y,c) # Number of docs in class c\n",
    "            self.prior.append(Nc/self.nDocs) # prior[c] = Nc/nDocs\n",
    "            hist_of_words_c = concatenateTextOfAllDocsInClass(X,y,c)\n",
    "            vocabulary_size = len(hist_of_words_c[hist_of_words_c > 0])\n",
    "            l = len(vocabulary)\n",
    "            for word in vocabulary :\n",
    "                word_index = vocabulary[word] #return the index of the word in the vocabulary\n",
    "                nTokens_c = countTokensOfTerm(hist_of_words_c,word_index) # return how many times the word appears in the class c docs\n",
    "                self.condProb[word_index,c] = (nTokens_c+1)/(np.sum(hist_of_words_c)+vocabulary_size) # update P(word|class) according to Laplace smoothing\n",
    "        return self.prior, self.condProb\n",
    "\n",
    "    def predict(self, X, allClasses = [0,1]):\n",
    "        print(\"Predicting  ...\")\n",
    "        array_score = np.zeros(X.shape[0])\n",
    "        last = 0 \n",
    "        for num_doc in range(X.shape[0]) :\n",
    "            if last != int(num_doc/X.shape[0]*100) :\n",
    "                last = int(num_doc/X.shape[0]*100)\n",
    "                #print(last, \" % done\", end = '\\r')\n",
    "            score = [0,0]\n",
    "            word_indexes = extractTokensFromDoc(X[num_doc,:])\n",
    "            for c in allClasses :\n",
    "                score[c] = np.log(self.prior[c]) # 0.5 for each\n",
    "                for word_index in list(word_indexes)[0] :\n",
    "                    score[c] += np.log(self.condProb[word_index,c]) # adding log of probabilities\n",
    "            array_score[num_doc] = np.argmax(np.array(score))\n",
    "        return array_score\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus of texts\n",
      "(2000, 50894)\n",
      "Training model ...\n",
      "Predicting  ...\n",
      "accuracy = 81.0 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' :\n",
    "    vocabulary, X = count_words(texts)\n",
    "    print(X.shape)\n",
    "    # Try to fit, predict and score\n",
    "    # We only split the dataset in two halves, future works will test a p-folds version.\n",
    "    nb = NB()\n",
    "    nb.fit(X[::2], y[::2])\n",
    "    print(\"accuracy = {acc} %\".format(acc = 100 * nb.score(X[1::2], y[1::2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y,n) :\n",
    "    train_size = int(X.shape[0]*(1-1/n))\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    training_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "    X_train, X_test, y_train, y_test = X[training_idx,:], X[test_idx,:], y[training_idx], y[test_idx]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X,y,n) :\n",
    "    nb = NB()\n",
    "    scores = []\n",
    "    for i in range(n):\n",
    "        print(\"Processing fold %d out of %d\"%(i+1,n))\n",
    "        X_train, X_test, y_train, y_test = split_data(X,y,n)\n",
    "        nb.fit(X_train,y_train)\n",
    "        acc = nb.score(X_test,y_test)\n",
    "        scores.append(acc)\n",
    "        print(\"accuracy = {n} % \\n\".format(n = 100*acc))\n",
    "    return np.mean(np.array(scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1 out of 5\n",
      "Training model ...\n",
      "Predicting  ...\n",
      "accuracy = 81.25 % \n",
      "\n",
      "Processing fold 2 out of 5\n",
      "Training model ...\n",
      "Predicting  ...\n",
      "accuracy = 81.0 % \n",
      "\n",
      "Processing fold 3 out of 5\n",
      "Training model ...\n",
      "Predicting  ...\n",
      "accuracy = 82.25 % \n",
      "\n",
      "Processing fold 4 out of 5\n",
      "Training model ...\n",
      "Predicting  ...\n",
      "accuracy = 85.0 % \n",
      "\n",
      "Processing fold 5 out of 5\n",
      "Training model ...\n",
      "Predicting  ...\n",
      "accuracy = 81.5 % \n",
      "\n",
      "mean accuracy = 82.2 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = cross_validation(X,y,5)\n",
    "print(\"mean accuracy = {n} % \\n\".format(n = 100*accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the word with the highest conditional probability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5, 0.5], array([[4.65155944e-06, 1.38731275e-06],\n",
       "        [1.55051981e-06, 6.93656374e-06],\n",
       "        [1.55051981e-06, 2.77462549e-06],\n",
       "        ...,\n",
       "        [1.55051981e-06, 2.77462549e-06],\n",
       "        [6.20207925e-06, 2.77462549e-06],\n",
       "        [3.10103962e-06, 2.77462549e-06]]))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_proba_stop_words(stop_words,vocabulary,model) :\n",
    "    indexes = [vocabulary[word] for word in stop_words if word in vocabulary]\n",
    "    max_proba = np.max(abs(model.condProb[indexes,1]-model.condProb[indexes,0]))\n",
    "    return indexes, max_proba\n",
    "\n",
    "stop_words = []\n",
    "nb = NB()\n",
    "nb.fit(X[::], y[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes, max_proba = max_proba_stop_words([open(f).read() for f in filenames_stop_words][0].split(),vocabulary,nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032954539444079722"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05406973 0.05730711]\n",
      "30941\n",
      "[0.05406973 0.05730711]\n",
      "30941\n"
     ]
    }
   ],
   "source": [
    "for index in indexes :\n",
    "    p = nb.condProb[index,:]\n",
    "    if np.max(p) > 0.05 :\n",
    "        print(p)\n",
    "        print(index)\n",
    "        \n",
    "for word in vocabulary : \n",
    "    p = nb.condProb[vocabulary[word],:]\n",
    "    if np.max(p) > 0.05 :\n",
    "        print(p)\n",
    "        print(vocabulary[word])\n",
    "        \n",
    "# the word nb 30941 has the highest conditional probability, and it is a stop_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary.keys() :\n",
    "    if vocabulary[word] == 30941 :\n",
    "        print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
